
import os
import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.prompts import PromptTemplate

# --- 1. í™˜ê²½ ì„¤ì • ë° ëª¨ë¸/DB ë¡œë“œ ---

os.environ['GOOGLE_API_KEY'] = st.secrets["GOOGLE_API_KEY"]

FAISS_CSV_PATH = "faiss_index_csv"
FAISS_PDF_PATH = "faiss_index_pdf"

@st.cache_resource
def load_models_and_db():
    print("í•œêµ­ì–´ íŠ¹í™” ìž„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
    embeddings = HuggingFaceEmbeddings(
        model_name="kykim/bert-kor-base",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    print("ìž„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")

    vectordb_csv, vectordb_pdf = None, None

    if os.path.exists(FAISS_CSV_PATH):
        vectordb_csv = FAISS.load_local(FAISS_CSV_PATH, embeddings, allow_dangerous_deserialization=True)
    if os.path.exists(FAISS_PDF_PATH):
        vectordb_pdf = FAISS.load_local(FAISS_PDF_PATH, embeddings, allow_dangerous_deserialization=True)
        
    return vectordb_csv, vectordb_pdf

# --- 2. ì•± ì‹¤í–‰ ë° UI êµ¬ì„± ---

st.set_page_config(page_title="ê³µë°±ì´ - ë¬´ë ¹ì™•ë¦‰ ì±—ë´‡", page_icon="ðŸº")
st.title("ðŸº ë¬´ë ¹ì™•ë¦‰ ìœ ë¬¼ ì •ë³´ ì±—ë´‡ 'ê³µë°±ì´'")

vectordb_csv, vectordb_pdf = load_models_and_db()

if not vectordb_csv:
    st.error("í•µì‹¬ ì§€ì‹ ë°ì´í„°ë² ì´ìŠ¤(`faiss_index_csv`)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
else:
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("ìœ ë¬¼ ì´ë¦„ì´ë‚˜ ì—­ì‚¬ì  ì‚¬ì‹¤ì„ ì§ˆë¬¸í•´ë³´ì„¸ìš”!"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.spinner("ì§€ì‹ì°½ê³ ë¥¼ ê²€ìƒ‰í•˜ê³  ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ìž…ë‹ˆë‹¤..."):
            
            # --- 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ---
            retriever_csv = vectordb_csv.as_retriever(search_kwargs={'k': 3})
            retrieved_docs_csv = retriever_csv.invoke(prompt)

            retrieved_docs_pdf = []
            if vectordb_pdf:
                retriever_pdf = vectordb_pdf.as_retriever(search_kwargs={'k': 3})
                retrieved_docs_pdf = retriever_pdf.invoke(prompt)

            all_retrieved_docs = retrieved_docs_csv + retrieved_docs_pdf

            # --- 4. ìˆ˜ë™ ì»¨í…ìŠ¤íŠ¸ ë° ëŒ€í™” ê¸°ë¡ êµ¬ì„± ---
            context = ""
            for doc in all_retrieved_docs:
                source = doc.metadata.get('source', 'N/A')
                if 'ì†Œìž¥í’ˆë²ˆí˜¸' in doc.metadata:
                    context_text = f"ìœ ë¬¼ëª…: {doc.metadata.get('ëª…ì¹­', '')}(ì†Œìž¥í’ˆë²ˆí˜¸: {doc.metadata.get('ì†Œìž¥í’ˆë²ˆí˜¸','')})\nìƒì„¸ ì„¤ëª…: {doc.metadata.get('íŠ¹ì§•', '')}"
                else:
                    context_text = doc.page_content
                context += f"--- [ì¶œì²˜: {source}] ---\n{context_text}\n\n"

            chat_history = ""
            # ë§ˆì§€ë§‰ 4ê°œì˜ ë©”ì‹œì§€(ì‚¬ìš©ìž2, ì±—ë´‡2)ë§Œ ê¸°ì–µì— í¬í•¨í•˜ì—¬ í† í° ì‚¬ìš©ëŸ‰ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
            for msg in st.session_state.messages[-4:]:
                chat_history += f"{msg['role']}: {msg['content']}\n"

            # --- 5. ìµœì¢… ë‹µë³€ ìƒì„± ---
            prompt_template = """ë‹¹ì‹ ì€ ì£¼ì–´ì§„ 'ê·¼ê±° ì •ë³´'ì™€ 'ëŒ€í™” ê¸°ë¡'ì„ ëª¨ë‘ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìžì˜ 'í˜„ìž¬ ì§ˆë¬¸'ì— ë‹µë³€í•˜ëŠ” ìœ ë¬¼ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.

            [ê·¼ê±° ì •ë³´]
            {context}

            [ëŒ€í™” ê¸°ë¡]
            {chat_history}

            [í˜„ìž¬ ì§ˆë¬¸]
            {question}

            [ì „ë¬¸ê°€ì˜ ë‹µë³€]
            """
            PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "chat_history", "question"])
            
            llm = ChatGoogleGenerativeAI(model="gemini-1.5-pro-latest", temperature=0.2)
            
            final_prompt = PROMPT.format(context=context, chat_history=chat_history, question=prompt)
            response = llm.invoke(final_prompt).content

            # --- 6. ìµœì¢… ê²°ê³¼ ì¶œë ¥ ---
            st.session_state.messages.append({"role": "assistant", "content": response})
            with st.chat_message("assistant"):
                st.markdown(response)

                # [ìˆ˜ì •ë¨] ê·¼ê±° ë¬¸ì„œ í‘œì‹œ ë¡œì§ì„ ë‹¤ì‹œ ì¶”ê°€í•˜ê³  ì™„ì„±í–ˆìŠµë‹ˆë‹¤.
                with st.expander("ë‹µë³€ ìƒì„±ì— ì‚¬ìš©ëœ ê·¼ê±° ë¬¸ì„œ í™•ì¸í•˜ê¸°"):
                    if not all_retrieved_docs:
                        st.write("ì´ë²ˆ ë‹µë³€ì— ì‚¬ìš©ëœ ê·¼ê±° ë¬¸ì„œëŠ” ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        for doc in all_retrieved_docs:
                            st.markdown(f"**ðŸ“‚ ì¶œì²˜:** `{doc.metadata.get('source', 'N/A')}`")
                            if 'ì†Œìž¥í’ˆë²ˆí˜¸' in doc.metadata:
                                st.markdown(f"**ðŸº ìœ ë¬¼ëª…:** `{doc.metadata.get('ëª…ì¹­', 'N/A')}`")
                            # page_contentëŠ” ê²€ìƒ‰ì— ì‚¬ìš©ëœ 'ìœ ë¬¼ëª…'ì´ë¯€ë¡œ, 'íŠ¹ì§•'ì„ ë³´ì—¬ì£¼ëŠ” ê²ƒì´ ë” ìœ ìš©í•©ë‹ˆë‹¤.
                            display_content = doc.metadata.get('íŠ¹ì§•', doc.page_content)
                            st.markdown(f"> {display_content[:200]}...")
                            st.markdown("---")